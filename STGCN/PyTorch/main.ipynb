{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e53716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from stgcn import STGCN\n",
    "from utils import generate_dataset, load_metr_la_data, get_normalized_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e31afb81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "use_gpu = False\n",
    "num_timesteps_input = 12\n",
    "num_timesteps_output = 3\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 50\n",
    "\n",
    "parser = argparse.ArgumentParser(description='STGCN')\n",
    "parser.add_argument('--enable-cuda', action='store_true',\n",
    "                    help='Enable CUDA')\n",
    "args = parser.parse_args()\n",
    "args.device = None\n",
    "if args.enable_cuda and torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "else:\n",
    "    args.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(training_input, training_target, batch_size):\n",
    "    \"\"\"\n",
    "    Trains one epoch with the given data.\n",
    "    :param training_input: Training inputs of shape (num_samples, num_nodes,\n",
    "    num_timesteps_train, num_features).\n",
    "    :param training_target: Training targets of shape (num_samples, num_nodes,\n",
    "    num_timesteps_predict).\n",
    "    :param batch_size: Batch size to use during training.\n",
    "    :return: Average loss for this epoch.\n",
    "    \"\"\"\n",
    "    permutation = torch.randperm(training_input.shape[0])\n",
    "\n",
    "    epoch_training_losses = []\n",
    "    for i in range(0, training_input.shape[0], batch_size):\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        X_batch, y_batch = training_input[indices], training_target[indices]\n",
    "        X_batch = X_batch.to(device=args.device)\n",
    "        y_batch = y_batch.to(device=args.device)\n",
    "\n",
    "        out = net(A_wave, X_batch)\n",
    "        loss = loss_criterion(out, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_training_losses.append(loss.detach().cpu().numpy())\n",
    "    return sum(epoch_training_losses)/len(epoch_training_losses)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(7)\n",
    "\n",
    "    A, X, means, stds = load_metr_la_data()\n",
    "\n",
    "    split_line1 = int(X.shape[2] * 0.6)\n",
    "    split_line2 = int(X.shape[2] * 0.8)\n",
    "\n",
    "    train_original_data = X[:, :, :split_line1]\n",
    "    val_original_data = X[:, :, split_line1:split_line2]\n",
    "    test_original_data = X[:, :, split_line2:]\n",
    "\n",
    "    training_input, training_target = generate_dataset(train_original_data,\n",
    "                                                       num_timesteps_input=num_timesteps_input,\n",
    "                                                       num_timesteps_output=num_timesteps_output)\n",
    "    val_input, val_target = generate_dataset(val_original_data,\n",
    "                                             num_timesteps_input=num_timesteps_input,\n",
    "                                             num_timesteps_output=num_timesteps_output)\n",
    "    test_input, test_target = generate_dataset(test_original_data,\n",
    "                                               num_timesteps_input=num_timesteps_input,\n",
    "                                               num_timesteps_output=num_timesteps_output)\n",
    "\n",
    "    A_wave = get_normalized_adj(A)\n",
    "    A_wave = torch.from_numpy(A_wave)\n",
    "\n",
    "    A_wave = A_wave.to(device=args.device)\n",
    "\n",
    "    net = STGCN(A_wave.shape[0],\n",
    "                training_input.shape[3],\n",
    "                num_timesteps_input,\n",
    "                num_timesteps_output).to(device=args.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    loss_criterion = nn.MSELoss()\n",
    "\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    validation_maes = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_epoch(training_input, training_target,\n",
    "                           batch_size=batch_size)\n",
    "        training_losses.append(loss)\n",
    "\n",
    "        # Run validation\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            val_input = val_input.to(device=args.device)\n",
    "            val_target = val_target.to(device=args.device)\n",
    "\n",
    "            out = net(A_wave, val_input)\n",
    "            val_loss = loss_criterion(out, val_target).to(device=\"cpu\")\n",
    "            validation_losses.append(np.asscalar(val_loss.detach().numpy()))\n",
    "\n",
    "            out_unnormalized = out.detach().cpu().numpy()*stds[0]+means[0]\n",
    "            target_unnormalized = val_target.detach().cpu().numpy()*stds[0]+means[0]\n",
    "            mae = np.mean(np.absolute(out_unnormalized - target_unnormalized))\n",
    "            validation_maes.append(mae)\n",
    "\n",
    "            out = None\n",
    "            val_input = val_input.to(device=\"cpu\")\n",
    "            val_target = val_target.to(device=\"cpu\")\n",
    "\n",
    "        print(\"Training loss: {}\".format(training_losses[-1]))\n",
    "        print(\"Validation loss: {}\".format(validation_losses[-1]))\n",
    "        print(\"Validation MAE: {}\".format(validation_maes[-1]))\n",
    "        plt.plot(training_losses, label=\"training loss\")\n",
    "        plt.plot(validation_losses, label=\"validation loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        checkpoint_path = \"checkpoints/\"\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            os.makedirs(checkpoint_path)\n",
    "        with open(\"checkpoints/losses.pk\", \"wb\") as fd:\n",
    "            pk.dump((training_losses, validation_losses, validation_maes), fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67436ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b536d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38535c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
